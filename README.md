
# ğŸ§  FastAPI + Gradio Chatbot with LLaMA 3 via Ollama

![App Running Screenshot](image/Running.png)

This project combines a FastAPI backend and a Gradio frontend to send prompts to the [LLaMA 3](https://ollama.com/library/llama3) model using [Ollama](https://ollama.com).

---

## ğŸš€ Requirements

- Python 3.8+
- [Ollama installed](https://ollama.com/download)
- LLaMA 3 model pulled via Ollama

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```
git clone https://github.com/quochn-RisingStar/FastAPI-Chat-API-with-LLaMA-3.git
cd my-ai-agent
```

### 2. Create and Activate a Virtual Environment

```
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows
```

### 3. Install Python Dependencies

```bash
pip install -r requirements.txt
```

Or manually:

```bash
pip install fastapi uvicorn gradio requests
```

---

## ğŸ§  Run the LLaMA 3 Model with Ollama

```bash
ollama pull llama3
ollama run llama3
```

> âœ… Ollama serves the model by default at `http://localhost:11434`.

---

## ğŸš€ Start the App (Backend + UI)

```bash
python3 app.py
```

> ğŸŸ¢ This will:
>
> * Start a FastAPI backend at: `http://127.0.0.1:8000`
> * Open a Gradio web UI at: [http://127.0.0.1:7860](http://127.0.0.1:7860)

---

## ğŸ§ª Test the API

```bash
curl -X POST http://localhost:8000/ask/stream \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Xin chÃ o"}'
```

> ğŸ—¨ï¸ The API streams back the LLaMA 3 response using FastAPI's `StreamingResponse`.

---

## ğŸ“ Project Structure

```
my-ai-agent/
â”œâ”€â”€ app.py               # Main entry point (FastAPI + Gradio)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âœ… `requirements.txt`

```txt
fastapi
uvicorn
gradio
requests
```

---

## ğŸ’¡ Ideas for Expansion

* âœ… Add prompt/response memory
* âœ… Add user chat history with timestamps
* ğŸ”’ Add API key or token authentication
* ğŸ“Š Monitor usage (requests per user)
* ğŸ§± Add SQLite/MongoDB to store conversations

---